{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Database Setup\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "def setup_database():\n",
    "    conn = sqlite3.connect('plant_disease_results.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS results (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        image_name TEXT,\n",
    "                        prediction TEXT,\n",
    "                        confidence REAL,\n",
    "                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "                    )''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "setup_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "IMAGE_SIZE=256\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2152 files belonging to 3 classes.\n",
      "Using 1722 files for training.\n",
      "Found 2152 files belonging to 3 classes.\n",
      "Using 430 files for validation.\n",
      "Training dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "Validation dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "dataset_path = 'D:/PROJECTS/4th Year Project/kaggle/PlantVillage'\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(dataset_path, img_size=(224, 224), batch_size=32, validation_split=0.2, seed=42):\n",
    "    # Load train and validation datasets using validation_split\n",
    "    train_ds = image_dataset_from_directory(\n",
    "        dataset_path,\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        subset=\"training\",\n",
    "        seed=seed\n",
    "    )\n",
    "    val_ds = image_dataset_from_directory(\n",
    "        dataset_path,\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        subset=\"validation\",\n",
    "        seed=seed\n",
    "    )\n",
    "    return train_ds, val_ds\n",
    "\n",
    "# Specify dataset path\n",
    "train_ds, val_ds = load_data(dataset_path)\n",
    "\n",
    "# Display the structure of datasets\n",
    "print(f\"Training dataset: {train_ds}\")\n",
    "print(f\"Validation dataset: {val_ds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m resize_and_rescale \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      2\u001b[0m   layers\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mResizing(IMAGE_SIZE, IMAGE_SIZE),\n\u001b[0;32m      3\u001b[0m   layers\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mRescaling(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m),\n\u001b[0;32m      4\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2152 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Data Augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create data augmentation generator\n",
    "def augment_data(train_ds, img_size=(224, 224)):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    augmented_data = datagen.flow_from_directory(\n",
    "        train_ds,\n",
    "        target_size=img_size,\n",
    "        batch_size=32,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    return augmented_data\n",
    "\n",
    "augmented_train_ds = augment_data(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n\u001b[0;32m      7\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     10\u001b[0m     resize_and_rescale,\n\u001b[0;32m     11\u001b[0m     layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m32\u001b[39m, kernel_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39minput_shape),\n\u001b[0;32m     12\u001b[0m     layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m     13\u001b[0m     layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m64\u001b[39m,  kernel_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     14\u001b[0m     layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m     15\u001b[0m     layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m64\u001b[39m,  kernel_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     16\u001b[0m     layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m     17\u001b[0m     layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     18\u001b[0m     layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m     19\u001b[0m     layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     20\u001b[0m     layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m     21\u001b[0m     layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     22\u001b[0m     layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m     23\u001b[0m     layers\u001b[38;5;241m.\u001b[39mFlatten(),\n\u001b[0;32m     24\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     25\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(n_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     26\u001b[0m ])\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mbuild(input_shape\u001b[38;5;241m=\u001b[39minput_shape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 4: Model Design\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Build the CNN model\n",
    "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = 3\n",
    "\n",
    "model = models.Sequential([\n",
    "    resize_and_rescale,\n",
    "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 907ms/step - accuracy: 0.4801 - loss: 826.4453 - val_accuracy: 0.6814 - val_loss: 0.9789\n",
      "Epoch 2/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 951ms/step - accuracy: 0.7367 - loss: 0.7033 - val_accuracy: 0.7116 - val_loss: 0.8542\n",
      "Epoch 3/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 906ms/step - accuracy: 0.7899 - loss: 0.6054 - val_accuracy: 0.7302 - val_loss: 0.7368\n",
      "Epoch 4/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 880ms/step - accuracy: 0.8404 - loss: 0.4731 - val_accuracy: 0.7349 - val_loss: 0.7294\n",
      "Epoch 5/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 874ms/step - accuracy: 0.8716 - loss: 0.4003 - val_accuracy: 0.7395 - val_loss: 0.7108\n",
      "Epoch 6/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 902ms/step - accuracy: 0.8950 - loss: 0.3198 - val_accuracy: 0.7233 - val_loss: 0.7563\n",
      "Epoch 7/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 872ms/step - accuracy: 0.9205 - loss: 0.2962 - val_accuracy: 0.7349 - val_loss: 0.8797\n",
      "Epoch 8/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 891ms/step - accuracy: 0.9092 - loss: 0.2860 - val_accuracy: 0.7372 - val_loss: 0.9450\n",
      "Epoch 9/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 854ms/step - accuracy: 0.9289 - loss: 0.2179 - val_accuracy: 0.7279 - val_loss: 1.1011\n",
      "Epoch 10/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 884ms/step - accuracy: 0.9413 - loss: 0.2067 - val_accuracy: 0.7442 - val_loss: 1.1042\n",
      "Epoch 11/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 866ms/step - accuracy: 0.9570 - loss: 0.1512 - val_accuracy: 0.7605 - val_loss: 1.1135\n",
      "Epoch 12/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 857ms/step - accuracy: 0.9607 - loss: 0.1152 - val_accuracy: 0.7558 - val_loss: 1.0854\n",
      "Epoch 13/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 854ms/step - accuracy: 0.9699 - loss: 0.1112 - val_accuracy: 0.7698 - val_loss: 1.4174\n",
      "Epoch 14/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 876ms/step - accuracy: 0.9632 - loss: 0.1329 - val_accuracy: 0.7605 - val_loss: 1.3676\n",
      "Epoch 15/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 927ms/step - accuracy: 0.9599 - loss: 0.1261 - val_accuracy: 0.7349 - val_loss: 1.1894\n",
      "Epoch 16/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 937ms/step - accuracy: 0.9679 - loss: 0.1178 - val_accuracy: 0.7674 - val_loss: 1.4927\n",
      "Epoch 17/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 825ms/step - accuracy: 0.9704 - loss: 0.1523 - val_accuracy: 0.7326 - val_loss: 1.4766\n",
      "Epoch 18/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 853ms/step - accuracy: 0.9709 - loss: 0.1168 - val_accuracy: 0.7535 - val_loss: 1.7174\n",
      "Epoch 19/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 853ms/step - accuracy: 0.9746 - loss: 0.0991 - val_accuracy: 0.7372 - val_loss: 1.8616\n",
      "Epoch 20/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 852ms/step - accuracy: 0.9713 - loss: 0.0934 - val_accuracy: 0.7512 - val_loss: 1.7758\n",
      "Epoch 21/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 828ms/step - accuracy: 0.9640 - loss: 0.1344 - val_accuracy: 0.7535 - val_loss: 1.6170\n",
      "Epoch 22/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 828ms/step - accuracy: 0.9851 - loss: 0.0531 - val_accuracy: 0.7302 - val_loss: 2.0356\n",
      "Epoch 23/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 911ms/step - accuracy: 0.9766 - loss: 0.0971 - val_accuracy: 0.7395 - val_loss: 1.7935\n",
      "Epoch 24/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 1s/step - accuracy: 0.9739 - loss: 0.0896 - val_accuracy: 0.7558 - val_loss: 1.5359\n",
      "Epoch 25/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.9816 - loss: 0.0716 - val_accuracy: 0.7488 - val_loss: 1.8018\n",
      "Epoch 26/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 1s/step - accuracy: 0.9783 - loss: 0.0801 - val_accuracy: 0.7558 - val_loss: 2.1000\n",
      "Epoch 27/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - accuracy: 0.9823 - loss: 0.0953 - val_accuracy: 0.7698 - val_loss: 1.5456\n",
      "Epoch 28/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.9834 - loss: 0.0690 - val_accuracy: 0.7488 - val_loss: 1.8915\n",
      "Epoch 29/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1s/step - accuracy: 0.9880 - loss: 0.0363 - val_accuracy: 0.7488 - val_loss: 2.0316\n",
      "Epoch 30/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 905ms/step - accuracy: 0.9827 - loss: 0.0757 - val_accuracy: 0.7698 - val_loss: 1.6240\n",
      "Epoch 31/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.9859 - loss: 0.0711 - val_accuracy: 0.7465 - val_loss: 2.2276\n",
      "Epoch 32/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - accuracy: 0.9905 - loss: 0.0462 - val_accuracy: 0.7721 - val_loss: 1.6512\n",
      "Epoch 33/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - accuracy: 0.9839 - loss: 0.0742 - val_accuracy: 0.7535 - val_loss: 1.9234\n",
      "Epoch 34/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.9843 - loss: 0.0551 - val_accuracy: 0.7349 - val_loss: 2.5139\n",
      "Epoch 35/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 975ms/step - accuracy: 0.9889 - loss: 0.0495 - val_accuracy: 0.7767 - val_loss: 1.7349\n",
      "Epoch 36/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 974ms/step - accuracy: 0.9890 - loss: 0.0676 - val_accuracy: 0.7698 - val_loss: 1.6678\n",
      "Epoch 37/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.9940 - loss: 0.0253 - val_accuracy: 0.7628 - val_loss: 2.0821\n",
      "Epoch 38/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.9892 - loss: 0.0405 - val_accuracy: 0.7721 - val_loss: 2.0367\n",
      "Epoch 39/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 844ms/step - accuracy: 0.9914 - loss: 0.0369 - val_accuracy: 0.7674 - val_loss: 1.9903\n",
      "Epoch 40/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 904ms/step - accuracy: 0.9794 - loss: 0.0723 - val_accuracy: 0.7721 - val_loss: 1.9933\n",
      "Epoch 41/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 868ms/step - accuracy: 0.9929 - loss: 0.0639 - val_accuracy: 0.7558 - val_loss: 3.0370\n",
      "Epoch 42/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 852ms/step - accuracy: 0.9925 - loss: 0.0562 - val_accuracy: 0.7767 - val_loss: 2.1550\n",
      "Epoch 43/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 912ms/step - accuracy: 0.9892 - loss: 0.0550 - val_accuracy: 0.7767 - val_loss: 2.1213\n",
      "Epoch 44/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 963ms/step - accuracy: 0.9919 - loss: 0.0335 - val_accuracy: 0.7884 - val_loss: 2.2631\n",
      "Epoch 45/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 949ms/step - accuracy: 0.9936 - loss: 0.0202 - val_accuracy: 0.7628 - val_loss: 2.3708\n",
      "Epoch 46/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 952ms/step - accuracy: 0.9868 - loss: 0.0605 - val_accuracy: 0.7767 - val_loss: 2.4003\n",
      "Epoch 47/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 891ms/step - accuracy: 0.9835 - loss: 0.0570 - val_accuracy: 0.7698 - val_loss: 2.0997\n",
      "Epoch 48/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 876ms/step - accuracy: 0.9932 - loss: 0.0302 - val_accuracy: 0.7767 - val_loss: 2.2335\n",
      "Epoch 49/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 906ms/step - accuracy: 0.9889 - loss: 0.0276 - val_accuracy: 0.7721 - val_loss: 2.4771\n",
      "Epoch 50/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 895ms/step - accuracy: 0.9901 - loss: 0.0337 - val_accuracy: 0.7651 - val_loss: 2.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Compile and train the model\n",
    "def train_model(model, train_ds, val_ds, epochs=50):\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',  # Use sparse loss\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "    model.save('plant_disease_model.h5')\n",
    "    return history\n",
    "\n",
    "history = train_model(model, train_ds, val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.83       182\n",
      "           1       0.79      0.75      0.77       208\n",
      "           2       0.44      0.30      0.36        40\n",
      "\n",
      "    accuracy                           0.77       430\n",
      "   macro avg       0.67      0.64      0.65       430\n",
      "weighted avg       0.76      0.77      0.76       430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the saved model\n",
    "def evaluate_model(model_path, val_ds):\n",
    "    model = load_model(model_path)\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for images, labels in val_ds:\n",
    "        preds = model.predict(images)\n",
    "        predictions.extend(np.argmax(preds, axis=1))\n",
    "        true_labels.extend(labels.numpy())  # No need for np.argmax here\n",
    "\n",
    "    print(classification_report(true_labels, predictions))\n",
    "\n",
    "evaluate_model('plant_disease_model.h5', val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Image: D:/PROJECTS/4th Year Project/kaggle/PlantVillage/Potato___Early_blight/0c5b14d9-8b1c-4c39-bb23-1835b5760caa___RS_Early.B 7937.JPG\n",
      "Predicted Class: Potato___healthy\n",
      "Confidence: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Testing with Individual Images\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def predict_image(model_path, img_path, class_names):\n",
    "    model = load_model(model_path)\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    confidence = predictions[0][predicted_class]\n",
    "\n",
    "    print(f\"Image: {img_path}\")\n",
    "    print(f\"Predicted Class: {class_names[predicted_class]}\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "\n",
    "    # Save results to the database\n",
    "    conn = sqlite3.connect('plant_disease_results.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''INSERT INTO results (image_name, prediction, confidence) VALUES (?, ?, ?)''',\n",
    "                   (os.path.basename(img_path), class_names[predicted_class], float(confidence)))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Example usage\n",
    "class_names = ['Potato___Early_blight', 'Potato___healthy', 'Potato___Late_blight']  # Replace with your actual class names\n",
    "predict_image('plant_disease_model.h5', 'D:/PROJECTS/4th Year Project/kaggle/PlantVillage/Potato___Early_blight/0c5b14d9-8b1c-4c39-bb23-1835b5760caa___RS_Early.B 7937.JPG', class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
